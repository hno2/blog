---
title: This Week I Learned - Week 34 2021
slug: til34
date: 2021-08-29 7:30:47 +0100
publications_src: content/til.bib
--- 


*What is this?* An semi-unstructured brain dump of things I read or learn - hopefully weekly. This is still short this week, way too much writing my thesis.

## Effect of the Week
Self-Serving bias suggests that nobody likes to admit to being incompetent. Instead, anything else than ourselves is likely to be blamed, especially external factors. We often use this automatically to present ourselves in a positive manner and thereby uphold our self-worth. When we believe we are in control of our success but external things are responsible for our failure, that's self-serving bias.

## AI 
This week seems to be all about cutting things from other things.

* It is crazy how far Music Demixing has come. Using [Spleeter](https://github.com/deezer/spleeter) by Deezer you can get astonishing results in separating voice from other components (*stems*) of music. You can easily create karaoke versions of songs or listen to only the voice. There was [Music Demixing challenge](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021) end of July hosted by Sony with a prize pool of 10k CHF.
* [Omnimatte](https://omnimatte.github.io/)[@@lu2021omnimatte] can remove contents from videos with surprising quality. It can even remove shadows or water deformation.

### Feedback Generation for student submitted Code
Educators from [Stanford University](http://ai.stanford.edu/blog/prototransformer/) [@@wu2021prototransformer] are one of the first or even the first to have deployed an AI system, that provides automatic and specific feedback to over 16,000 students coding solutions. As costs of manual annotation are labor-intensive, due to the diverse nature of students solutions [@wu2021prototransformer] frames the feedback challenge as a few-shot classification problem. They contribute a meta-learning framework for a few-shot classification of sequence data like programming code to build a novel network architecture called ProtoTransformer.

![ProtoTransformer Architecture (Graphic by [@wu2021prototransformer] )](../images/weekly/prototransformer.svg)

They employ [few-shot learning](/n-shot) to classify rubrics of feedback to give and use the model's attention to highlight the problematic sections. Side Information about task description and existing rubrics is added by embedding it via a pretrained S-BERT Model [@@reimers2019sentence]. Code strings are tokenized, variable names normalized via byte-pair encoding (BPE), and the pretrained model weights of CodeBERT are used for the main network.

## Psychology
* It seems like people's[ decision-making approaches](https://www.psychologistworld.com/cognitive/maximizers-satisficers-decision-making) tend to fit into one of two categories: either you are a maximizer – a person who is committed to making a choice that will ultimately benefit them as much as possible – or you are a satisfying person whose choices are based on less exact criteria and more on consideration on what they wish to gain. [@@simon1956rational]. Having more choices does not make us happier with an outcome of a decision. This shows for example in online dating where maximizers want to exhaust options to find the optimal partner. In practice, this leads to the more-means-worse effect, where more searching leads to worse choices by distracting with irrelevant information.[@@yang2010looking]